{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Implicit Bias of Optimization in Deep Networks\n",
                "\n",
                "Interactive analysis notebook for exploring implicit bias of different optimizers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, '..')\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "from src.models import LinearModel, ShallowMLP, DeepMLP\n",
                "from src.optimizers import get_optimizer, FullBatchGD, Lion\n",
                "from src.metrics import (\n",
                "    MetricsTracker, compute_weight_norms, compute_margin,\n",
                "    compute_accuracy, compute_margin_ratios\n",
                ")\n",
                "from src.data import (\n",
                "    generate_linearly_separable, generate_xor_data,\n",
                "    generate_spiral_data, get_dataset\n",
                ")\n",
                "from src.visualization import (\n",
                "    plot_training_curves, plot_decision_boundary,\n",
                "    plot_multi_decision_boundaries, plot_margin_comparison\n",
                ")\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "%matplotlib inline\n",
                "\n",
                "print('Setup complete!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Quick Demo: Linear Model on Separable Data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate data\n",
                "X, y = generate_linearly_separable(n_samples=100, dim=2, margin=1.0, seed=42)\n",
                "\n",
                "# Visualize\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.scatter(X[y > 0, 0], X[y > 0, 1], c='blue', label='+1', edgecolors='white')\n",
                "plt.scatter(X[y < 0, 0], X[y < 0, 1], c='red', label='-1', edgecolors='white')\n",
                "plt.xlabel('x₁')\n",
                "plt.ylabel('x₂')\n",
                "plt.title('Linearly Separable Data')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_and_compare(X, y, epochs=2000, lr=0.01):\n",
                "    \"\"\"Train with multiple optimizers and compare.\"\"\"\n",
                "    optimizers = ['gd', 'sgd', 'adam', 'lion']\n",
                "    results = {}\n",
                "    models = {}\n",
                "    \n",
                "    for opt_name in optimizers:\n",
                "        # Fresh model\n",
                "        model = LinearModel(input_dim=X.shape[1], output_dim=1, bias=False)\n",
                "        torch.manual_seed(42)\n",
                "        nn.init.normal_(model.linear.weight, std=0.01)\n",
                "        \n",
                "        optimizer = get_optimizer(opt_name, model.parameters(), lr=lr)\n",
                "        loss_fn = nn.BCEWithLogitsLoss()\n",
                "        y_bce = (y + 1) / 2\n",
                "        \n",
                "        history = {'loss': [], 'margin': [], 'norm_l2': [], 'norm_linf': []}\n",
                "        \n",
                "        for epoch in range(epochs):\n",
                "            optimizer.zero_grad()\n",
                "            outputs = model(X).squeeze()\n",
                "            loss = loss_fn(outputs, y_bce)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            if epoch % 100 == 0:\n",
                "                l2, linf = compute_weight_norms(model)\n",
                "                margin = compute_margin(model, X, y, normalize=True)\n",
                "                history['loss'].append(loss.item())\n",
                "                history['margin'].append(margin)\n",
                "                history['norm_l2'].append(l2)\n",
                "                history['norm_linf'].append(linf)\n",
                "        \n",
                "        results[opt_name] = history\n",
                "        models[opt_name] = model\n",
                "        \n",
                "        l2_ratio, linf_ratio = compute_margin_ratios(model, X, y)\n",
                "        print(f\"{opt_name.upper():>5}: ℓ2-ratio={l2_ratio:.4f}, ℓ∞-ratio={linf_ratio:.4f}\")\n",
                "    \n",
                "    return results, models\n",
                "\n",
                "results, models = train_and_compare(X, y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Plot training dynamics\n",
                "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
                "\n",
                "for opt_name, history in results.items():\n",
                "    epochs = range(0, len(history['loss']) * 100, 100)\n",
                "    axes[0, 0].plot(epochs, history['loss'], label=opt_name.upper())\n",
                "    axes[0, 1].plot(epochs, history['margin'], label=opt_name.upper())\n",
                "    axes[1, 0].plot(epochs, history['norm_l2'], label=opt_name.upper())\n",
                "    axes[1, 1].plot(epochs, history['norm_linf'], label=opt_name.upper())\n",
                "\n",
                "axes[0, 0].set_title('Training Loss'); axes[0, 0].set_xlabel('Epoch'); axes[0, 0].legend()\n",
                "axes[0, 1].set_title('Normalized Margin'); axes[0, 1].set_xlabel('Epoch'); axes[0, 1].legend()\n",
                "axes[1, 0].set_title('Weight Norm (ℓ₂)'); axes[1, 0].set_xlabel('Epoch'); axes[1, 0].legend()\n",
                "axes[1, 1].set_title('Weight Norm (ℓ∞)'); axes[1, 1].set_xlabel('Epoch'); axes[1, 1].legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Decision boundaries comparison\n",
                "plot_multi_decision_boundaries(models, X, y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. XOR Data (Requires Nonlinearity)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X_xor, y_xor = generate_xor_data(n_samples=200, noise=0.15, seed=42)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "plt.scatter(X_xor[y_xor > 0, 0], X_xor[y_xor > 0, 1], c='blue', label='+1')\n",
                "plt.scatter(X_xor[y_xor < 0, 0], X_xor[y_xor < 0, 1], c='red', label='-1')\n",
                "plt.title('XOR Data')\n",
                "plt.legend()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train shallow MLP on XOR\n",
                "def train_mlp(X, y, hidden_dim=32, epochs=1000, lr=0.01):\n",
                "    models = {}\n",
                "    \n",
                "    for opt_name in ['gd', 'adam', 'lion']:\n",
                "        model = ShallowMLP(input_dim=2, hidden_dim=hidden_dim, activation='relu')\n",
                "        torch.manual_seed(42)\n",
                "        \n",
                "        optimizer = get_optimizer(opt_name, model.parameters(), lr=lr)\n",
                "        loss_fn = nn.BCEWithLogitsLoss()\n",
                "        y_bce = (y + 1) / 2\n",
                "        \n",
                "        for epoch in range(epochs):\n",
                "            optimizer.zero_grad()\n",
                "            loss = loss_fn(model(X).squeeze(), y_bce)\n",
                "            loss.backward()\n",
                "            optimizer.step()\n",
                "        \n",
                "        acc = compute_accuracy(model, X, y)\n",
                "        print(f\"{opt_name.upper()}: Accuracy = {acc:.2%}\")\n",
                "        models[opt_name] = model\n",
                "    \n",
                "    return models\n",
                "\n",
                "xor_models = train_mlp(X_xor, y_xor)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_multi_decision_boundaries(xor_models, X_xor, y_xor)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load and Analyze Experiment Results\n",
                "\n",
                "After running the experiment scripts, load and analyze results here."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "from pathlib import Path\n",
                "\n",
                "# Load baseline results if available\n",
                "baseline_path = Path('../results/baseline/all_metrics.json')\n",
                "if baseline_path.exists():\n",
                "    with open(baseline_path) as f:\n",
                "        baseline_results = json.load(f)\n",
                "    print('Loaded baseline results!')\n",
                "    plot_training_curves(baseline_results)\n",
                "else:\n",
                "    print('Run experiments/01_baseline_linear.py first!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Custom Experiments\n",
                "\n",
                "Use this section to run custom experiments."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your custom experiments here\n",
                "pass"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
